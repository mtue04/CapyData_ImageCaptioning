{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_caption_path = '../data/US-Capydata-ViSportIC/normalized_dataset/train/captions.txt'\n",
    "val_caption_path = '../data/US-Capydata-ViSportIC/normalized_dataset/val/captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_captions(file_path):\n",
    "    captions = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Tách đường dẫn ảnh và caption\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                image_path = parts[0]\n",
    "                caption = parts[1]\n",
    "                \n",
    "                # Trích xuất tên file ảnh từ đường dẫn\n",
    "                image_filename = os.path.basename(image_path)\n",
    "                \n",
    "                captions.append({\n",
    "                    'image_path': image_path,\n",
    "                    'image_filename': image_filename,\n",
    "                    'caption': caption\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(captions)\n",
    "\n",
    "df_train = read_captions(train_caption_path)\n",
    "df_val = read_captions(val_caption_path)\n",
    "caption_df = pd.concat([df_train, df_val], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_caption(caption):\n",
    "    tokenized_captions = []\n",
    "    \n",
    "    for idx, row in caption_df.iterrows():\n",
    "        caption = row['caption']\n",
    "        \n",
    "        # Tách caption thành các token (dựa vào khoảng trắng)\n",
    "        tokens = caption.strip().split()\n",
    "        \n",
    "        # Tạo lại caption từ tokens\n",
    "        tokenized_caption = ' '.join(tokens)\n",
    "        \n",
    "        tokenized_captions.append(tokenized_caption)\n",
    "    \n",
    "    caption_df['tokenized_caption'] = tokenized_captions\n",
    "    return caption_df\n",
    "\n",
    "caption_df = tokenize_caption(caption_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1403\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(caption_df, min_word_freq=5):\n",
    "    # Đếm tần suất xuất hiện của các từ\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for caption in caption_df['tokenized_caption']:\n",
    "        words = caption.split()\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    # Thêm token đặc biệt\n",
    "    vocab = {\n",
    "        '<pad>': 0,  # Padding\n",
    "        '<unk>': 1,  # Unknown\n",
    "        '<startseq>': 2,  # Bắt đầu câu \n",
    "        '<endseq>': 3  # Kết thúc câu\n",
    "    }\n",
    "    \n",
    "    # Thêm các từ có tần suất xuất hiện lớn hơn ngưỡng\n",
    "    idx = len(vocab)\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_word_freq and word not in vocab:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    # Tạo mapping ngược từ index sang từ\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    return vocab, idx_to_word, word_counts\n",
    "\n",
    "vocab, idx_to_word, word_counts = build_vocabulary(caption_df, min_word_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to ..\\vocabulary\n"
     ]
    }
   ],
   "source": [
    "def save_vocabulary(vocab, idx_to_word, word_counts, save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # Lưu vocab\n",
    "    with open(os.path.join(save_path, 'vocab.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Lưu idx_to_word\n",
    "    with open(os.path.join(save_path, 'idx_to_word.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(idx_to_word, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Lưu word_counts\n",
    "    with open(os.path.join(save_path, 'word_counts.pkl'), 'wb') as f:\n",
    "        pickle.dump(dict(word_counts), f)\n",
    "    \n",
    "    # Lưu danh sách các từ theo tần suất xuất hiện (để phân tích)\n",
    "    word_freq_df = pd.DataFrame({\n",
    "        'word': list(word_counts.keys()),\n",
    "        'frequency': list(word_counts.values())\n",
    "    }).sort_values('frequency', ascending=False)\n",
    "    \n",
    "    word_freq_df.to_csv(os.path.join(save_path, 'word_frequencies.csv'), \n",
    "                         index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Vocabulary saved to {save_path}\")\n",
    "\n",
    "save_vocabulary(vocab, idx_to_word, word_counts, '..\\\\vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_captions(caption_df, vocab):\n",
    "    encoded_captions = []\n",
    "    \n",
    "    for caption in caption_df['tokenized_caption']:\n",
    "        words = caption.split()\n",
    "        # Chuyển mỗi từ thành index tương ứng trong vocab\n",
    "        indices = [vocab.get(word, vocab['<unk>']) for word in words]\n",
    "        encoded_captions.append(indices)\n",
    "    \n",
    "    caption_df['encoded_caption'] = encoded_captions\n",
    "    return caption_df\n",
    "\n",
    "caption_df = encode_captions(caption_df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_caption_mapping(caption_df):\n",
    "    image_to_captions = {}\n",
    "    \n",
    "    for idx, row in caption_df.iterrows():\n",
    "        image_filename = row['image_filename']\n",
    "        caption = row['tokenized_caption']\n",
    "        encoded_caption = row['encoded_caption']\n",
    "        \n",
    "        if image_filename not in image_to_captions:\n",
    "            image_to_captions[image_filename] = []\n",
    "        \n",
    "        image_to_captions[image_filename].append({\n",
    "            'caption': caption,\n",
    "            'encoded_caption': encoded_caption\n",
    "        })\n",
    "    \n",
    "    return image_to_captions\n",
    "\n",
    "image_caption_map = create_image_caption_mapping(caption_df)\n",
    "\n",
    "# Lưu map\n",
    "with open('../vocabulary/image_caption_map.pkl', 'wb') as f:\n",
    "    pickle.dump(image_caption_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số từ trong vocabulary: 1403\n",
      "Độ dài caption: Min = 6, Max = 33, Trung bình = 12.79\n",
      "Top 20 từ phổ biến nhất:\n",
      "  đang: 16614\n",
      "  một: 7942\n",
      "  tennis: 7629\n",
      "  người: 7504\n",
      "  trên: 7479\n",
      "  bóng: 7268\n",
      "  sân: 6139\n",
      "  vận_động_viên: 4594\n",
      "  cầu_thủ: 4420\n",
      "  bóng_chày: 4276\n",
      "  ở: 3766\n",
      "  đàn_ông: 3758\n",
      "  vợt: 3715\n",
      "  cầm: 2966\n",
      "  để: 2852\n",
      "  đánh_bóng: 2717\n",
      "  chơi: 2608\n",
      "  trong: 2473\n",
      "  và: 2214\n",
      "  những: 2146\n",
      "Số lượng từ hiếm (xuất hiện < 5 lần): 2310\n"
     ]
    }
   ],
   "source": [
    "def analyze_vocabulary(vocab, word_counts):\n",
    "    print(f\"Tổng số từ trong vocabulary: {len(vocab)}\")\n",
    "    \n",
    "    # Phân tích độ dài caption\n",
    "    caption_lengths = [len(caption.split()) for caption in caption_df['tokenized_caption']]\n",
    "    max_length = max(caption_lengths)\n",
    "    min_length = min(caption_lengths)\n",
    "    avg_length = sum(caption_lengths) / len(caption_lengths)\n",
    "    \n",
    "    print(f\"Độ dài caption: Min = {min_length}, Max = {max_length}, Trung bình = {avg_length:.2f}\")\n",
    "    \n",
    "    # Top 20 từ phổ biến nhất (loại bỏ <startseq> và <endseq>)\n",
    "    most_common = [item for item in word_counts.most_common(22) if item[0] not in ['<startseq>', '<endseq>']][:20]\n",
    "    print(\"Top 20 từ phổ biến nhất:\")\n",
    "    for word, count in most_common:\n",
    "        print(f\"  {word}: {count}\")\n",
    "    \n",
    "    # Số lượng từ hiếm (xuất hiện ít hơn ngưỡng)\n",
    "    rare_words = sum(1 for word, count in word_counts.items() if count < 5)\n",
    "    print(f\"Số lượng từ hiếm (xuất hiện < 5 lần): {rare_words}\")\n",
    "\n",
    "analyze_vocabulary(vocab, word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py\n",
      "  Downloading h5py-3.13.0-cp313-cp313-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\mtue2\\onedrive - vnu-hcmus\\university\\s8\\text mining\\capydata_imagecaptioning\\capydata_image_captioning\\lib\\site-packages (from h5py) (2.2.3)\n",
      "Downloading h5py-3.13.0-cp313-cp313-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.9 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 9.7 MB/s eta 0:00:00\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ../vocabulary/processed_captions.h5\n"
     ]
    }
   ],
   "source": [
    "def prepare_training_data(caption_df, output_file):\n",
    "    import h5py\n",
    "    \n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        # Lưu danh sách tên file ảnh\n",
    "        image_filenames = caption_df['image_filename'].tolist()\n",
    "        f.create_dataset('image_filenames', data=[name.encode('utf-8') for name in image_filenames])\n",
    "        \n",
    "        # Lưu encoded captions\n",
    "        # Padded để đảm bảo cùng độ dài\n",
    "        caption_lengths = [len(cap) for cap in caption_df['encoded_caption']]\n",
    "        max_length = max(caption_lengths)\n",
    "        \n",
    "        padded_captions = []\n",
    "        for cap in caption_df['encoded_caption']:\n",
    "            # Padding với 0 (<pad>) đến max_length\n",
    "            padded_cap = cap + [vocab['<pad>']] * (max_length - len(cap))\n",
    "            padded_captions.append(padded_cap)\n",
    "        \n",
    "        f.create_dataset('captions', data=padded_captions)\n",
    "        f.create_dataset('caption_lengths', data=caption_lengths)\n",
    "    \n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "prepare_training_data(caption_df, '../vocabulary/processed_captions.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capydata_image_captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
