{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing misunderstanding about train ViT and CNN... I do not know when it is training it will automatically preprocess images into suitable ones. So I will adjust file a little bit. **THIS FILE WILL NOT BE REMOVED IN ORDER TO GENERATING REPORT!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# remove ./dataset in self_crawl, UIT-ViIC and US_Capydata-ViSportIC\n",
    "folders_to_remove = [\n",
    "    '../data/self_crawl/dataset',\n",
    "    '../data/UIT-ViIC/dataset',\n",
    "    '../data/US-Capydata-ViSportIC/dataset'\n",
    "]\n",
    "\n",
    "for folder in folders_to_remove:\n",
    "    shutil.rmtree(folder, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove preprocessed images\\\n",
    "folders_to_remove = [\n",
    "    '../data/US-Capydata-ViSportIC/preprocessed_vit',\n",
    "    '../data/US-Capydata-ViSportIC/preprocessed_cnn',\n",
    "]\n",
    "\n",
    "for folder in folders_to_remove:\n",
    "    shutil.rmtree(folder, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "captions_path = \"../data/US-Capydata-ViSportIC/processed_captions.txt\"\n",
    "images_path = \"../data/US-Capydata-ViSportIC/images\"\n",
    "\n",
    "# output paths\n",
    "train_captions_path = \"../data/US-Capydata-ViSportIC/dataset/train/captions.txt\"\n",
    "val_captions_path = \"../data/US-Capydata-ViSportIC/dataset/val/captions.txt\"\n",
    "test_captions_path = \"../data/US-Capydata-ViSportIC/dataset/test/captions.txt\"\n",
    "\n",
    "train_images_path = \"../data/US-Capydata-ViSportIC/dataset/train/images\"\n",
    "val_images_path = \"../data/US-Capydata-ViSportIC/dataset/val/images\"\n",
    "test_images_path = \"../data/US-Capydata-ViSportIC/dataset/test/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [train_images_path, val_images_path, test_images_path]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "for path in [train_captions_path, val_captions_path, test_captions_path]:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Thiết lập seed để đảm bảo tính tái tạo\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Đọc file caption\n",
    "caption_df = pd.read_csv(captions_path, sep='\\t', names=['image_filename', 'caption'], encoding='utf-8')\n",
    "\n",
    "# Nhóm các caption theo ảnh\n",
    "image_to_captions = {}\n",
    "for _, row in caption_df.iterrows():\n",
    "    img_name = row['image_filename']\n",
    "    caption = row['caption']\n",
    "    \n",
    "    if img_name not in image_to_captions:\n",
    "        image_to_captions[img_name] = []\n",
    "    \n",
    "    image_to_captions[img_name].append(caption)\n",
    "\n",
    "# Lấy danh sách ảnh từ thư mục và đảm bảo chúng tồn tại trong caption\n",
    "image_filenames = os.listdir(images_path)\n",
    "image_filenames = [img for img in image_filenames if img in image_to_captions]\n",
    "\n",
    "# Phân chia ảnh ngẫu nhiên\n",
    "random.shuffle(image_filenames)\n",
    "num_images = len(image_filenames)\n",
    "train_count = int(num_images * train_ratio)\n",
    "val_count = int(num_images * val_ratio)\n",
    "test_count = num_images - train_count - val_count\n",
    "\n",
    "train_images = image_filenames[:train_count]\n",
    "val_images = image_filenames[train_count:train_count + val_count]\n",
    "test_images = image_filenames[train_count + val_count:]\n",
    "\n",
    "# Lưu caption vào các file tương ứng\n",
    "def save_captions(image_list, captions_dict, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for img_name in image_list:\n",
    "            if img_name in captions_dict:\n",
    "                for caption in captions_dict[img_name]:\n",
    "                    f.write(f\"{img_name}\\t{caption}\\n\")\n",
    "\n",
    "save_captions(train_images, image_to_captions, train_captions_path)\n",
    "save_captions(val_images, image_to_captions, val_captions_path)\n",
    "save_captions(test_images, image_to_captions, test_captions_path)\n",
    "\n",
    "# Sao chép ảnh vào các thư mục tương ứng\n",
    "def copy_images(image_list, src_folder, dest_folder):\n",
    "    for img_name in image_list:\n",
    "        src_path = os.path.join(src_folder, img_name)\n",
    "        dest_path = os.path.join(dest_folder, img_name)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "copy_images(train_images, images_path, train_images_path)\n",
    "copy_images(val_images, images_path, val_images_path)\n",
    "copy_images(test_images, images_path, test_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been split into train, val, and test sets.\n",
      "Train set: 3446 images\n",
      "Validation set: 738 images\n",
      "Test set: 739 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset has been split into train, val, and test sets.\")\n",
    "print(f\"Train set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(val_images)} images\")\n",
    "print(f\"Test set: {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self_crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_path = \"../data/self_crawl/processed_captions.txt\"\n",
    "images_path = \"../data/self_crawl/images\"\n",
    "\n",
    "# output paths\n",
    "train_captions_path = \"../data/self_crawl/dataset/train/captions.txt\"\n",
    "val_captions_path = \"../data/self_crawl/dataset/val/captions.txt\"\n",
    "test_captions_path = \"../data/self_crawl/dataset/test/captions.txt\"\n",
    "\n",
    "train_images_path = \"../data/self_crawl/dataset/train/images\"\n",
    "val_images_path = \"../data/self_crawl/dataset/val/images\"\n",
    "test_images_path = \"../data/self_crawl/dataset/test/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [train_images_path, val_images_path, test_images_path]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "for path in [train_captions_path, val_captions_path, test_captions_path]:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Thiết lập seed để đảm bảo tính tái tạo\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Đọc file caption\n",
    "caption_df = pd.read_csv(captions_path, sep='\\t', names=['image_filename', 'caption'], encoding='utf-8')\n",
    "\n",
    "# Nhóm các caption theo ảnh\n",
    "image_to_captions = {}\n",
    "for _, row in caption_df.iterrows():\n",
    "    img_name = row['image_filename']\n",
    "    caption = row['caption']\n",
    "    \n",
    "    if img_name not in image_to_captions:\n",
    "        image_to_captions[img_name] = []\n",
    "    \n",
    "    image_to_captions[img_name].append(caption)\n",
    "\n",
    "# Lấy danh sách ảnh từ thư mục và đảm bảo chúng tồn tại trong caption\n",
    "image_filenames = os.listdir(images_path)\n",
    "image_filenames = [img for img in image_filenames if img in image_to_captions]\n",
    "\n",
    "# Phân chia ảnh ngẫu nhiên\n",
    "random.shuffle(image_filenames)\n",
    "num_images = len(image_filenames)\n",
    "train_count = int(num_images * train_ratio)\n",
    "val_count = int(num_images * val_ratio)\n",
    "test_count = num_images - train_count - val_count\n",
    "\n",
    "train_images = image_filenames[:train_count]\n",
    "val_images = image_filenames[train_count:train_count + val_count]\n",
    "test_images = image_filenames[train_count + val_count:]\n",
    "\n",
    "# Lưu caption vào các file tương ứng\n",
    "def save_captions(image_list, captions_dict, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for img_name in image_list:\n",
    "            if img_name in captions_dict:\n",
    "                for caption in captions_dict[img_name]:\n",
    "                    f.write(f\"{img_name}\\t{caption}\\n\")\n",
    "\n",
    "save_captions(train_images, image_to_captions, train_captions_path)\n",
    "save_captions(val_images, image_to_captions, val_captions_path)\n",
    "save_captions(test_images, image_to_captions, test_captions_path)\n",
    "\n",
    "# Sao chép ảnh vào các thư mục tương ứng\n",
    "def copy_images(image_list, src_folder, dest_folder):\n",
    "    for img_name in image_list:\n",
    "        src_path = os.path.join(src_folder, img_name)\n",
    "        dest_path = os.path.join(dest_folder, img_name)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "copy_images(train_images, images_path, train_images_path)\n",
    "copy_images(val_images, images_path, val_images_path)\n",
    "copy_images(test_images, images_path, test_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been split into train, val, and test sets.\n",
      "Train set: 753 images\n",
      "Validation set: 161 images\n",
      "Test set: 162 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset has been split into train, val, and test sets.\")\n",
    "print(f\"Train set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(val_images)} images\")\n",
    "print(f\"Test set: {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UIT-ViIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_path = \"../data/UIT-ViIC/processed_captions.txt\"\n",
    "images_path = \"../data/UIT-ViIC/images\"\n",
    "\n",
    "# output paths\n",
    "train_captions_path = \"../data/UIT-ViIC/dataset/train/captions.txt\"\n",
    "val_captions_path = \"../data/UIT-ViIC/dataset/val/captions.txt\"\n",
    "test_captions_path = \"../data/UIT-ViIC/dataset/test/captions.txt\"\n",
    "\n",
    "train_images_path = \"../data/UIT-ViIC/dataset/train/images\"\n",
    "val_images_path = \"../data/UIT-ViIC/dataset/val/images\"\n",
    "test_images_path = \"../data/UIT-ViIC/dataset/test/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [train_images_path, val_images_path, test_images_path]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "for path in [train_captions_path, val_captions_path, test_captions_path]:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Thiết lập seed để đảm bảo tính tái tạo\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Đọc file caption\n",
    "caption_df = pd.read_csv(captions_path, sep='\\t', names=['image_filename', 'caption'], encoding='utf-8')\n",
    "\n",
    "# Nhóm các caption theo ảnh\n",
    "image_to_captions = {}\n",
    "for _, row in caption_df.iterrows():\n",
    "    img_name = row['image_filename']\n",
    "    caption = row['caption']\n",
    "    \n",
    "    if img_name not in image_to_captions:\n",
    "        image_to_captions[img_name] = []\n",
    "    \n",
    "    image_to_captions[img_name].append(caption)\n",
    "\n",
    "# Lấy danh sách ảnh từ thư mục và đảm bảo chúng tồn tại trong caption\n",
    "image_filenames = os.listdir(images_path)\n",
    "image_filenames = [img for img in image_filenames if img in image_to_captions]\n",
    "\n",
    "# Phân chia ảnh ngẫu nhiên\n",
    "random.shuffle(image_filenames)\n",
    "num_images = len(image_filenames)\n",
    "train_count = int(num_images * train_ratio)\n",
    "val_count = int(num_images * val_ratio)\n",
    "test_count = num_images - train_count - val_count\n",
    "\n",
    "train_images = image_filenames[:train_count]\n",
    "val_images = image_filenames[train_count:train_count + val_count]\n",
    "test_images = image_filenames[train_count + val_count:]\n",
    "\n",
    "# Lưu caption vào các file tương ứng\n",
    "def save_captions(image_list, captions_dict, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for img_name in image_list:\n",
    "            if img_name in captions_dict:\n",
    "                for caption in captions_dict[img_name]:\n",
    "                    f.write(f\"{img_name}\\t{caption}\\n\")\n",
    "\n",
    "save_captions(train_images, image_to_captions, train_captions_path)\n",
    "save_captions(val_images, image_to_captions, val_captions_path)\n",
    "save_captions(test_images, image_to_captions, test_captions_path)\n",
    "\n",
    "# Sao chép ảnh vào các thư mục tương ứng\n",
    "def copy_images(image_list, src_folder, dest_folder):\n",
    "    for img_name in image_list:\n",
    "        src_path = os.path.join(src_folder, img_name)\n",
    "        dest_path = os.path.join(dest_folder, img_name)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "copy_images(train_images, images_path, train_images_path)\n",
    "copy_images(val_images, images_path, val_images_path)\n",
    "copy_images(test_images, images_path, test_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been split into train, val, and test sets.\n",
      "Train set: 2692 images\n",
      "Validation set: 577 images\n",
      "Test set: 578 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset has been split into train, val, and test sets.\")\n",
    "print(f\"Train set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(val_images)} images\")\n",
    "print(f\"Test set: {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuilt vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def read_captions(file_path):\n",
    "    captions = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Tách đường dẫn ảnh và caption\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                image_path = parts[0]\n",
    "                caption = parts[1]\n",
    "                \n",
    "                captions.append({\n",
    "                    'image_path': image_path,\n",
    "                    'caption': caption\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(captions)\n",
    "\n",
    "def tokenize_caption(caption_df):\n",
    "    tokenized_captions = []\n",
    "    \n",
    "    for idx, row in caption_df.iterrows():\n",
    "        caption = row['caption']\n",
    "        \n",
    "        # Tách caption thành các token (dựa vào khoảng trắng)\n",
    "        tokens = caption.strip().split()\n",
    "        \n",
    "        # Tạo lại caption từ tokens\n",
    "        tokenized_caption = ' '.join(tokens)\n",
    "        \n",
    "        tokenized_captions.append(tokenized_caption)\n",
    "    \n",
    "    caption_df['tokenized_caption'] = tokenized_captions\n",
    "    return caption_df\n",
    "\n",
    "def build_vocabulary(caption_df, min_word_freq=5):\n",
    "    # Đếm tần suất xuất hiện của các từ\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for caption in caption_df['tokenized_caption']:\n",
    "        words = caption.split()\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    # Thêm token đặc biệt\n",
    "    vocab = {\n",
    "        '<pad>': 0,  # Padding\n",
    "        '<unk>': 1,  # Unknown\n",
    "        '<s>': 2,  # Bắt đầu câu \n",
    "        '</s>': 3  # Kết thúc câu\n",
    "    }\n",
    "    \n",
    "    # Thêm các từ có tần suất xuất hiện lớn hơn ngưỡng\n",
    "    idx = len(vocab)\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_word_freq and word not in vocab:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    # Tạo mapping ngược từ index sang từ\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    return vocab, idx_to_word, word_counts\n",
    "\n",
    "def save_vocabulary(vocab, idx_to_word, word_counts, save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # Lưu vocab\n",
    "    with open(os.path.join(save_path, 'vocab.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Lưu idx_to_word\n",
    "    with open(os.path.join(save_path, 'idx_to_word.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(idx_to_word, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Lưu word_counts\n",
    "    with open(os.path.join(save_path, 'word_counts.pkl'), 'wb') as f:\n",
    "        pickle.dump(dict(word_counts), f)\n",
    "    \n",
    "    # Lưu danh sách các từ theo tần suất xuất hiện (để phân tích)\n",
    "    word_freq_df = pd.DataFrame({\n",
    "        'word': list(word_counts.keys()),\n",
    "        'frequency': list(word_counts.values())\n",
    "    }).sort_values('frequency', ascending=False)\n",
    "    \n",
    "    word_freq_df.to_csv(os.path.join(save_path, 'word_frequencies.csv'), \n",
    "                         index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Vocabulary saved to {save_path}\")\n",
    "\n",
    "def encode_captions(caption_df, vocab):\n",
    "    encoded_captions = []\n",
    "    \n",
    "    for caption in caption_df['tokenized_caption']:\n",
    "        words = caption.split()\n",
    "        # Chuyển mỗi từ thành index tương ứng trong vocab\n",
    "        indices = [vocab.get(word, vocab['<unk>']) for word in words]\n",
    "        encoded_captions.append(indices)\n",
    "    \n",
    "    caption_df['encoded_caption'] = encoded_captions\n",
    "    return caption_df\n",
    "\n",
    "def create_image_caption_mapping(caption_df):\n",
    "    image_to_captions = {}\n",
    "    \n",
    "    for idx, row in caption_df.iterrows():\n",
    "        image_path = row['image_path']  # Thay image_filename bằng image_path\n",
    "        caption = row['tokenized_caption']\n",
    "        encoded_caption = row['encoded_caption']\n",
    "        \n",
    "        if image_path not in image_to_captions:\n",
    "            image_to_captions[image_path] = []\n",
    "        \n",
    "        image_to_captions[image_path].append({\n",
    "            'caption': caption,\n",
    "            'encoded_caption': encoded_caption\n",
    "        })\n",
    "    \n",
    "    return image_to_captions\n",
    "\n",
    "def analyze_vocabulary(vocab, word_counts, caption_df):\n",
    "    print(f\"Tổng số từ trong vocabulary: {len(vocab)}\")\n",
    "    \n",
    "    # Phân tích độ dài caption\n",
    "    caption_lengths = [len(caption.split()) for caption in caption_df['tokenized_caption']]\n",
    "    max_length = max(caption_lengths)\n",
    "    min_length = min(caption_lengths)\n",
    "    avg_length = sum(caption_lengths) / len(caption_lengths)\n",
    "    \n",
    "    print(f\"Độ dài caption: Min = {min_length}, Max = {max_length}, Trung bình = {avg_length:.2f}\")\n",
    "    \n",
    "    # Top 20 từ phổ biến nhất (loại bỏ <s> và </s>)\n",
    "    most_common = [item for item in word_counts.most_common(22) if item[0] not in ['<s>', '</s>']][:20]\n",
    "    print(\"Top 20 từ phổ biến nhất:\")\n",
    "    for word, count in most_common:\n",
    "        print(f\"  {word}: {count}\")\n",
    "    \n",
    "    # Số lượng từ hiếm (xuất hiện ít hơn ngưỡng)\n",
    "    rare_words = sum(1 for word, count in word_counts.items() if count < 5)\n",
    "    print(f\"Số lượng từ hiếm (xuất hiện < 5 lần): {rare_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary for US-Capydata-ViSportIC...\n",
      "Vocabulary size: 1402\n",
      "Vocabulary saved to ../data/US-Capydata-ViSportIC/vocab\n",
      "Tổng số từ trong vocabulary: 1402\n",
      "Độ dài caption: Min = 5, Max = 31, Trung bình = 10.94\n",
      "Top 20 từ phổ biến nhất:\n",
      "  đang: 16515\n",
      "  bóng: 9904\n",
      "  một: 7936\n",
      "  tennis: 7657\n",
      "  người: 7558\n",
      "  trên: 7434\n",
      "  sân: 6159\n",
      "  vận_động_viên: 4652\n",
      "  cầu_thủ: 4293\n",
      "  bóng_chày: 4132\n",
      "  ở: 3825\n",
      "  đàn_ông: 3752\n",
      "  vợt: 3718\n",
      "  đánh: 3518\n",
      "  cầm: 3062\n",
      "  để: 2802\n",
      "  chơi: 2601\n",
      "  trong: 2561\n",
      "  và: 2233\n",
      "  chuẩn_bị: 2199\n",
      "Số lượng từ hiếm (xuất hiện < 5 lần): 2157\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary for US-Capydata-ViSportIC\n",
    "print(\"Building vocabulary for US-Capydata-ViSportIC...\")\n",
    "train_caption_us = \"../data/US-Capydata-ViSportIC/dataset/train/captions.txt\"\n",
    "val_caption_us = \"../data/US-Capydata-ViSportIC/dataset/val/captions.txt\"\n",
    "\n",
    "df_train_us = read_captions(train_caption_us)\n",
    "df_val_us = read_captions(val_caption_us)\n",
    "df_us = pd.concat([df_train_us, df_val_us])\n",
    "\n",
    "# Tokenize captions\n",
    "df_us = tokenize_caption(df_us.copy())\n",
    "\n",
    "# Build vocabulary\n",
    "vocab_us, idx_to_word_us, word_counts_us = build_vocabulary(df_us, min_word_freq=5)\n",
    "\n",
    "# Save vocabulary\n",
    "save_path_us = \"../data/US-Capydata-ViSportIC/vocab\"\n",
    "os.makedirs(save_path_us, exist_ok=True)\n",
    "save_vocabulary(vocab_us, idx_to_word_us, word_counts_us, save_path_us)\n",
    "\n",
    "# Save mapping\n",
    "df_us = encode_captions(df_us, vocab_us)\n",
    "\n",
    "image_caption_mapping_us = create_image_caption_mapping(df_us)\n",
    "with open(os.path.join(save_path_us, 'image_caption_mapping.pkl'), 'wb') as f:\n",
    "    pickle.dump(image_caption_mapping_us, f)\n",
    "\n",
    "analyze_vocabulary(vocab_us, word_counts_us, df_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary for UIT-ViIC...\n",
      "Vocabulary size: 549\n",
      "Vocabulary saved to ../data/UIT-ViIC/vocab\n",
      "Tổng số từ trong vocabulary: 549\n",
      "Độ dài caption: Min = 4, Max = 31, Trung bình = 10.04\n",
      "Top 20 từ phổ biến nhất:\n",
      "  đang: 15213\n",
      "  bóng: 9598\n",
      "  tennis: 7650\n",
      "  người: 7141\n",
      "  trên: 6849\n",
      "  một: 6531\n",
      "  sân: 5900\n",
      "  bóng_chày: 4257\n",
      "  cầu_thủ: 4041\n",
      "  vợt: 3723\n",
      "  ở: 3708\n",
      "  đàn_ông: 3701\n",
      "  đánh: 3432\n",
      "  vận_động_viên: 3172\n",
      "  cầm: 3069\n",
      "  để: 2659\n",
      "  chơi: 2531\n",
      "  quả: 1887\n",
      "  thi_đấu: 1775\n",
      "  phụ_nữ: 1739\n",
      "Số lượng từ hiếm (xuất hiện < 5 lần): 860\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary for UIT-ViIC\n",
    "print(\"Building vocabulary for UIT-ViIC...\")\n",
    "train_caption_uit = \"../data/UIT-ViIC/dataset/train/captions.txt\"\n",
    "val_caption_uit = \"../data/UIT-ViIC/dataset/val/captions.txt\"\n",
    "\n",
    "df_train_uit = read_captions(train_caption_uit)\n",
    "df_val_uit = read_captions(val_caption_uit)\n",
    "df_uit = pd.concat([df_train_uit, df_val_uit])\n",
    "\n",
    "# Tokenize captions\n",
    "df_uit = tokenize_caption(df_uit.copy())\n",
    "\n",
    "# Build vocabulary\n",
    "vocab_uit, idx_to_word_uit, word_counts_uit = build_vocabulary(df_uit, min_word_freq=5)\n",
    "\n",
    "# Save vocabulary\n",
    "save_path_uit = \"../data/UIT-ViIC/vocab\"\n",
    "os.makedirs(save_path_uit, exist_ok=True)\n",
    "save_vocabulary(vocab_uit, idx_to_word_uit, word_counts_uit, save_path_uit)\n",
    "\n",
    "# Save mapping\n",
    "df_uit = encode_captions(df_uit, vocab_uit)\n",
    "\n",
    "image_caption_mapping_uit = create_image_caption_mapping(df_uit)\n",
    "with open(os.path.join(save_path_uit, 'image_caption_mapping.pkl'), 'wb') as f:\n",
    "    pickle.dump(image_caption_mapping_uit, f)\n",
    "\n",
    "analyze_vocabulary(vocab_uit, word_counts_uit, df_uit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary for self_crawl...\n",
      "Vocabulary size: 1122\n",
      "Vocabulary saved to ../data/self_crawl/vocab\n",
      "Tổng số từ trong vocabulary: 1122\n",
      "Độ dài caption: Min = 8, Max = 23, Trung bình = 14.11\n",
      "Top 20 từ phổ biến nhất:\n",
      "  vận_động_viên: 1471\n",
      "  trong: 1409\n",
      "  đang: 1397\n",
      "  một: 1248\n",
      "  cú: 868\n",
      "  và: 858\n",
      "  của: 842\n",
      "  đầy: 805\n",
      "  những: 779\n",
      "  sự: 767\n",
      "  với: 757\n",
      "  cho: 720\n",
      "  đấu: 681\n",
      "  trên: 617\n",
      "  thể_hiện: 615\n",
      "  này: 542\n",
      "  chuẩn_bị: 518\n",
      "  trận: 506\n",
      "  mạnh_mẽ: 474\n",
      "  khi: 454\n",
      "Số lượng từ hiếm (xuất hiện < 5 lần): 1708\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary for self_crawl\n",
    "print(\"Building vocabulary for self_crawl...\")\n",
    "train_caption_self_crawl = \"../data/self_crawl/dataset/train/captions.txt\"\n",
    "val_caption_self_crawl = \"../data/self_crawl/dataset/val/captions.txt\"\n",
    "\n",
    "df_train_self_crawl = read_captions(train_caption_self_crawl)\n",
    "df_val_self_crawl = read_captions(val_caption_self_crawl)\n",
    "df_self_crawl = pd.concat([df_train_self_crawl, df_val_self_crawl])\n",
    "\n",
    "# Tokenize captions\n",
    "df_self_crawl = tokenize_caption(df_self_crawl.copy())\n",
    "\n",
    "# Build vocabulary\n",
    "vocab_self, idx_to_word_self, word_counts_self = build_vocabulary(df_self_crawl, min_word_freq=5)\n",
    "\n",
    "# Save vocabulary\n",
    "save_path_self_crawl = \"../data/self_crawl/vocab\"\n",
    "os.makedirs(save_path_self_crawl, exist_ok=True)\n",
    "save_vocabulary(vocab_self, idx_to_word_self, word_counts_self, save_path_self_crawl)\n",
    "\n",
    "# Save mapping\n",
    "df_self_crawl = encode_captions(df_self_crawl, vocab_self)\n",
    "\n",
    "image_caption_mapping_self_crawl = create_image_caption_mapping(df_self_crawl)\n",
    "with open(os.path.join(save_path_self_crawl, 'image_caption_mapping.pkl'), 'wb') as f:\n",
    "    pickle.dump(image_caption_mapping_self_crawl, f)\n",
    "\n",
    "analyze_vocabulary(vocab_self, word_counts_self, df_self_crawl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "us-capydata-visportic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
