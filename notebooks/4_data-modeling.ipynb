{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we use three popular metrics- BLEU-4, ROUGE-L and GPT-Score, to assess the model's performance.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "- Reference captions: List of ground truth captions.\n",
    "\n",
    "- Hypothesis caption: Generated caption from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ BLEU-4 (Bilingual Evaluation Understudy)\n",
    "\n",
    "- BLEU measures n-gram overlaps between the generated caption and reference captions.\n",
    "\n",
    "- BLEU-4 specifically considers 4-grams to ensure both lexical and syntactic accuracy.\n",
    "\n",
    "General BLEU formula:\n",
    "\n",
    "$$ BLEU = BP \\times \\exp \\left( \\sum_{n=1}^{4} w_n \\log p_n \\right) $$\n",
    "\n",
    "Where:\n",
    "- $BP$ (Brevity Penalty) penalizes short captions.\n",
    "\n",
    "- $p_n$ represents the n-gram match ratio.\n",
    "\n",
    "- $w_n$ is the weight for each n-gram (BLEU-4: $w_n$ for n = 1 to 4).\n",
    "\n",
    "\n",
    "To easily compute, we use `nltk.translate.bleu_score` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "# Ensure required NLTK packages are downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(references, hypotheses, total=True):\n",
    "    \"\"\"\n",
    "    Compute BLEU-4 score.\n",
    "    :param references: List of reference captions (each reference is a list of tokenized captions)\n",
    "    :param hypotheses: List of hypothesis captions (each hypothesis is a tokenized caption)\n",
    "    :param total: If True, compute BLEU score for all hypotheses against all references\n",
    "                  If False, compute BLEU score for each hypothesis against its reference\n",
    "    :return: BLEU-4 score or list of BLEU-4 scores for each hypothesis\n",
    "    \"\"\"\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    if total:\n",
    "        # Compute BLEU score for all hypotheses against all references\n",
    "        score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "        return score\n",
    "\n",
    "    bleu_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        # Compute BLEU score for each hypothesis against its reference\n",
    "        score = corpus_bleu([ref], [hyp], smoothing_function=smoothie)\n",
    "        bleu_scores.append(score)\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total BLEU-4 score:\n",
      "BLEU-4 score: 0.9896\n",
      "\n",
      "Individual BLEU-4 scores:\n",
      "Hypothesis 1: BLEU-4 score: 0.9802\n",
      "Hypothesis 2: BLEU-4 score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "text_references = [\n",
    "    [\"váº­n_Ä‘á»™ng_viÃªn chuáº©n_bá»‹ phÃ¡t bÃ³ng\", \"cáº§u_thá»§ chuáº©n_bá»‹ Ä‘Ã¡ bÃ³ng\"],\n",
    "    [\"cáº§u_thá»§ Ä‘ang cháº¡y\", \"váº­n_Ä‘á»™ng_viÃªn Ä‘ang cháº¡y\"]\n",
    "]\n",
    "text_hypotheses = [\n",
    "    \"cáº§u_thá»§ chuáº©n_bá»‹ phÃ¡t bÃ³ng\",\n",
    "    \"váº­n_Ä‘á»™ng_viÃªn Ä‘ang cháº¡y\"\n",
    "]\n",
    "print(\"Total BLEU-4 score:\")\n",
    "bleu_score = compute_bleu(text_references, text_hypotheses)\n",
    "print(f\"BLEU-4 score: {bleu_score:.4f}\")\n",
    "\n",
    "print(\"\\nIndividual BLEU-4 scores:\")\n",
    "bleu_scores = compute_bleu(text_references, text_hypotheses, total=False)\n",
    "for i, score in enumerate(bleu_scores):\n",
    "    print(f\"Hypothesis {i+1}: BLEU-4 score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "- ROUGE evaluates the longest common subsequence (LCS) between two texts.\n",
    "\n",
    "ROUGE-L formula:\n",
    "\n",
    "$$ ROUGE-L = \\frac{LCS(reference, hypothesis)}{\\text{max length}} $$\n",
    "\n",
    "Where:\n",
    "- Recall: $\\frac{LCS}{\\text{number of words in ref}}$\n",
    "- Precision: $\\frac{LCS}{\\text{number of words in hypo}}$\n",
    "- F1-score: Harmonic mean of Recall and Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.2.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from rouge_score) (1.21.6)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from nltk->rouge_score) (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Downloading absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py): started\n",
      "  Building wheel for rouge_score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24972 sha256=1e6c4b61fbe02db709de8fcc28936aa61806a0594f42a3a65b93ec3374981004\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\5f\\dd\\89\\461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: absl-py, rouge_score\n",
      "Successfully installed absl-py-2.2.1 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(references, hypotheses, total=True):\n",
    "    \"\"\"\n",
    "    Compute ROUGE-L score.\n",
    "    :param references: List of reference captions (each reference is a list of tokenized captions)\n",
    "    :param hypotheses: List of hypothesis captions (each hypothesis is a tokenized caption)\n",
    "    :param total: If True, compute ROUGE-L score for all hypotheses against all references\n",
    "                  If False, compute ROUGE-L score for each hypothesis against its reference\n",
    "    :return: ROUGE-L score or list of ROUGE-L scores for each hypothesis\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    if total:\n",
    "        scores = []\n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            max_score = max(scorer.score(ref, hyp)['rougeL'].fmeasure for ref in refs)\n",
    "            scores.append(max_score)\n",
    "        return sum(scores) / len(scores)\n",
    "    \n",
    "    # Compute ROUGE score for each hypothesis against its reference\n",
    "    rouge_scores = []\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        max_score = max(scorer.score(ref, hyp)['rougeL'].fmeasure for ref in refs)\n",
    "        rouge_scores.append(max_score)\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ROUGE-L score:\n",
      "ROUGE-L score: 0.9444\n",
      "\n",
      "Individual ROUGE-L scores:\n",
      "Hypothesis 1: ROUGE-L score: 0.8889\n",
      "Hypothesis 2: ROUGE-L score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "text_references = [\n",
    "    [\"váº­n_Ä‘á»™ng_viÃªn chuáº©n_bá»‹ phÃ¡t bÃ³ng\", \"cáº§u_thá»§ chuáº©n_bá»‹ Ä‘Ã¡ bÃ³ng\"],\n",
    "    [\"cáº§u_thá»§ Ä‘ang cháº¡y\", \"váº­n_Ä‘á»™ng_viÃªn Ä‘ang cháº¡y\"]\n",
    "]\n",
    "text_hypotheses = [\n",
    "    \"cáº§u_thá»§ chuáº©n_bá»‹ phÃ¡t bÃ³ng\",\n",
    "    \"váº­n_Ä‘á»™ng_viÃªn Ä‘ang cháº¡y\"\n",
    "]\n",
    "\n",
    "print(\"Total ROUGE-L score:\")\n",
    "rouge_score = compute_rouge(text_references, text_hypotheses)\n",
    "print(f\"ROUGE-L score: {rouge_score:.4f}\")\n",
    "\n",
    "print(\"\\nIndividual ROUGE-L scores:\")\n",
    "rouge_scores = compute_rouge(text_references, text_hypotheses, total=False)\n",
    "for i, score in enumerate(rouge_scores):\n",
    "    print(f\"Hypothesis {i+1}: ROUGE-L score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ BERTScore (Bidirectional Encoder Representations from Transformers Score)\n",
    "\n",
    "- BERTScore evaluates semantic similarity by comparing contextualized embeddings from a transformer-based model.\n",
    "\n",
    "- Instead of exact word matching, BERTScore considers deep contextual meaning.\n",
    "\n",
    "- It calculates:\n",
    "    + Precision: Similarity between predicted and reference embeddings.\n",
    "    + Recall: Similarity between reference and predicted embeddings.\n",
    "    + F1-score: Harmonic mean of Precision and Recall.\n",
    "\n",
    "- BERTScore is especially useful for Vietnamese, as it captures meaning beyond exact word matches.\n",
    "\n",
    "To simply implement, we use `evaluate.load(\"bertscore\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (1.21.6)\n",
      "Requirement already satisfied: dill in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (1.4.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (4.66.6)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from pandas->evaluate) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from bert_score) (2.6.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from bert_score) (1.4.2)\n",
      "Collecting transformers>=3.0.0 (from bert_score)\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from bert_score) (1.21.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from bert_score) (4.66.6)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from bert_score) (3.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from bert_score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from tqdm>=4.31.1->bert_score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=3.0.0->bert_score)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=3.0.0->bert_score)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from matplotlib->bert_score) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from matplotlib->bert_score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from matplotlib->bert_score) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from requests->bert_score) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from requests->bert_score) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from requests->bert_score) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\miniconda3\\envs\\min_ds-env2\\lib\\site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.1)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/10.2 MB 9.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.7/10.2 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.2 MB 9.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.2 MB 8.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.7/10.2 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 7.9 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.6/2.4 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 6.9 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, tokenizers, transformers, bert_score\n",
      "Successfully installed bert_score-0.3.13 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load BERTScore\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(references, hypotheses, lang=\"vi\", total=True):\n",
    "    \"\"\"\n",
    "    Compute BERTScore.\n",
    "    :param references: List of reference captions (each reference is a list of captions)\n",
    "    :param hypotheses: List of hypothesis captions\n",
    "    :param lang: Language for BERTScore (default is Vietnamese \"vi\")\n",
    "    :param total: If True, compute BERTScore for all hypotheses against all references\n",
    "                  If False, compute BERTScore for each hypothesis against its reference\n",
    "    :return: Average BERTScore F1-score\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        best_ref = max(refs, key=len)  # Choose the longest reference per hypothesis\n",
    "        score = bertscore.compute(predictions=[hyp], references=[best_ref], lang=lang, device=\"cpu\")['f1'][0]\n",
    "        scores.append(score)\n",
    "    if total: \n",
    "        return sum(scores) / len(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total BERTScore:\n",
      "BERTScore: 0.9549\n",
      "\n",
      "Individual BERTScores:\n",
      "Hypothesis 1: BERTScore: 0.9099\n",
      "Hypothesis 2: BERTScore: 1.0000\n"
     ]
    }
   ],
   "source": [
    "text_references = [\n",
    "    [\"váº­n_Ä‘á»™ng_viÃªn chuáº©n_bá»‹ phÃ¡t bÃ³ng\", \"cáº§u_thá»§ chuáº©n_bá»‹ Ä‘Ã¡ bÃ³ng\"],\n",
    "    [\"cáº§u_thá»§ Ä‘ang cháº¡y\", \"váº­n_Ä‘á»™ng_viÃªn Ä‘ang cháº¡y\"]\n",
    "]\n",
    "text_hypotheses = [\n",
    "    \"cáº§u_thá»§ chuáº©n_bá»‹ phÃ¡t bÃ³ng\",\n",
    "    \"váº­n_Ä‘á»™ng_viÃªn Ä‘ang cháº¡y\"\n",
    "]\n",
    "\n",
    "print(\"Total BERTScore:\")\n",
    "bertscore_score = compute_bertscore(text_references, text_hypotheses)\n",
    "print(f\"BERTScore: {bertscore_score:.4f}\")\n",
    "\n",
    "print(\"\\nIndividual BERTScores:\")\n",
    "bertscore_scores = compute_bertscore(text_references, text_hypotheses, total=False)\n",
    "for i, score in enumerate(bertscore_scores):\n",
    "    print(f\"Hypothesis {i+1}: BERTScore: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
